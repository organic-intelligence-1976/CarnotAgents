{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph + MCP Agent System with Real LLMs\n",
    "\n",
    "This notebook demonstrates using real LLMs from different providers with the agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable async support in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import the agent system and LLM providers\n",
    "from agent_system import Agent, Tool, AgentSystem, create_python_tool, create_memory_tool\n",
    "from llm_providers import create_llm, create_llm_for_use_case, RECOMMENDED_MODELS\n",
    "import asyncio\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API key found\n",
      "✓ Anthropic API key found\n",
      "✓ Google API key found\n",
      "✗ Ollama not running (start with 'ollama serve')\n"
     ]
    }
   ],
   "source": [
    "# Check which API keys are available\n",
    "available_providers = []\n",
    "\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    available_providers.append('openai')\n",
    "    print(\"✓ OpenAI API key found\")\n",
    "else:\n",
    "    print(\"✗ OpenAI API key not found\")\n",
    "\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    available_providers.append('anthropic')\n",
    "    print(\"✓ Anthropic API key found\")\n",
    "else:\n",
    "    print(\"✗ Anthropic API key not found\")\n",
    "\n",
    "if os.getenv('GOOGLE_API_KEY'):\n",
    "    available_providers.append('gemini')\n",
    "    print(\"✓ Google API key found\")\n",
    "else:\n",
    "    print(\"✗ Google API key not found\")\n",
    "\n",
    "# Check if Ollama is running\n",
    "try:\n",
    "    import requests\n",
    "    response = requests.get('http://localhost:11434/api/tags')\n",
    "    if response.status_code == 200:\n",
    "        available_providers.append('ollama')\n",
    "        print(\"✓ Ollama is running locally\")\n",
    "        models = response.json().get('models', [])\n",
    "        if models:\n",
    "            print(f\"  Available models: {', '.join([m['name'] for m in models])}\")\n",
    "except:\n",
    "    print(\"✗ Ollama not running (start with 'ollama serve')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Agents with Different LLM Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created OpenAI agent with GPT-4-turbo\n",
      "Created Anthropic agent with Claude-3-Opus\n",
      "Created Gemini agent\n"
     ]
    }
   ],
   "source": [
    "# Create tools\n",
    "def calculator_tool(expression: str) -> str:\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "calc_tool = Tool(\n",
    "    name=\"calculator\",\n",
    "    func=calculator_tool,\n",
    "    description=\"Evaluates mathematical expressions\"\n",
    ")\n",
    "\n",
    "# Create agents with different LLMs\n",
    "agents = {}\n",
    "\n",
    "# OpenAI Agent (if available)\n",
    "if 'openai' in available_providers:\n",
    "    agents['openai'] = Agent(\n",
    "        name=\"OpenAI_Assistant\",\n",
    "        context={\"specialty\": \"general assistance with OpenAI models\"},\n",
    "        tools=[calc_tool],\n",
    "        llm_config={\n",
    "            \"provider\": \"openai\",\n",
    "            \"model\": \"gpt-4-turbo-preview\",\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    print(\"Created OpenAI agent with GPT-4-turbo\")\n",
    "\n",
    "# Anthropic Agent (if available)\n",
    "if 'anthropic' in available_providers:\n",
    "    agents['anthropic'] = Agent(\n",
    "        name=\"Claude_Assistant\",\n",
    "        context={\"specialty\": \"thoughtful analysis with Claude\"},\n",
    "        tools=[calc_tool],\n",
    "        llm_config={\n",
    "            \"provider\": \"anthropic\",\n",
    "            \"model\": \"claude-3-opus-20240229\",\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    print(\"Created Anthropic agent with Claude-3-Opus\")\n",
    "\n",
    "# Gemini Agent (if available)\n",
    "if 'gemini' in available_providers:\n",
    "    agents['gemini'] = Agent(\n",
    "        name=\"Gemini_Assistant\",\n",
    "        context={\"specialty\": \"Google's Gemini model insights\"},\n",
    "        tools=[calc_tool],\n",
    "        llm_config={\n",
    "            \"provider\": \"gemini\",\n",
    "            \"model\": \"gemini-1.5-flash\",\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    print(\"Created Gemini agent\")\n",
    "\n",
    "# Ollama Agent (if available)\n",
    "if 'ollama' in available_providers:\n",
    "    agents['ollama'] = Agent(\n",
    "        name=\"Local_Assistant\",\n",
    "        context={\"specialty\": \"local Llama model processing\"},\n",
    "        tools=[calc_tool],\n",
    "        llm_config={\n",
    "            \"provider\": \"ollama\",\n",
    "            \"model\": \"llama2\",  # or \"mistral\", \"codellama\", etc.\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    print(\"Created Ollama agent with Llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     agents.pop('anthropic')\n",
    "# except:\n",
    "#     pass\n",
    "# try:\n",
    "#     agents.pop('openai')\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing openai agent...\n",
      "============================================================\n",
      "Agent: OpenAI_Assistant\n",
      "LLM Provider: ChatOpenAI\n",
      "Tools Used: []\n",
      "\n",
      "Response:\n",
      "To compute \\(15 \\times 23 + 42\\), you first multiply 15 by 23, which equals 345. Then, you add 42 to the result of the multiplication.\n",
      "\n",
      "\\[15 \\times 23 = 345\\]\n",
      "\n",
      "\\[345 + 42 = 387\\]\n",
      "\n",
      "Therefore, \\(15 \\times 23 + 42 = 387\\).\n",
      "\n",
      "============================================================\n",
      "Testing anthropic agent...\n",
      "============================================================\n",
      "Agent: Claude_Assistant\n",
      "LLM Provider: ChatAnthropic\n",
      "Tools Used: []\n",
      "\n",
      "Response:\n",
      "To calculate 15 * 23 + 42 without using a calculator:\n",
      "\n",
      "First, multiply 15 by 23:\n",
      "  23\n",
      "x 15\n",
      "----\n",
      " 115\n",
      " 230\n",
      "----\n",
      " 345\n",
      "\n",
      "So, 15 * 23 = 345\n",
      "\n",
      "Then, add 42 to the result:\n",
      "\n",
      "345 + 42 = 387\n",
      "\n",
      "Therefore, 15 * 23 + 42 equals 387.\n",
      "\n",
      "============================================================\n",
      "Testing gemini agent...\n",
      "============================================================\n",
      "Agent: Gemini_Assistant\n",
      "LLM Provider: ChatGoogleGenerativeAI\n",
      "Tools Used: []\n",
      "\n",
      "Response:\n",
      "The calculation 15 * 23 + 42 was not performed using a calculator as requested.  Therefore, I cannot provide a numerical result.  To obtain the answer, the calculation needs to be performed manually or with a calculator.\n"
     ]
    }
   ],
   "source": [
    "# Test the agents with a simple task\n",
    "test_task = \"Please use the calculator tool to compute 15 * 23 + 42\"\n",
    "\n",
    "for provider, agent in agents.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {provider} agent...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = await agent.run(test_task)\n",
    "    \n",
    "    print(f\"Agent: {result['agent']}\")\n",
    "    print(f\"LLM Provider: {result['llm_provider']}\")\n",
    "    print(f\"Tools Used: {result['tools_used']}\")\n",
    "    print(f\"\\nResponse:\\n{result['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Create a tool that's clearly needed\ndef large_number_calculator(expression: str) -> str:\n    \"\"\"Handles calculations with very large numbers that LLMs struggle with\"\"\"\n    try:\n        result = eval(expression)\n        return f\"Exact result: {result:,}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\nlarge_calc_tool = Tool(\n    name=\"large_calculator\",\n    func=large_number_calculator,\n    description=\"Calculates exact results for large numbers and complex expressions\"\n)\n\n# Test with a calculation that clearly needs the tool\nlarge_number_task = \"Use the large_calculator tool to find the exact value of 123456789 * 987654321\"\n\nprint(\"Testing tool usage with large numbers:\")\nfor provider in ['openai', 'gemini']:\n    if provider in available_providers:\n        agent = Agent(\n            name=f\"{provider}_calculator\",\n            context={\"instruction\": \"You must use tools when explicitly asked\"},\n            tools=[large_calc_tool],\n            llm_config={\"provider\": provider, \"temperature\": 0.3}\n        )\n        \n        result = await agent.run(large_number_task)\n        print(f\"\\n{provider.upper()}:\")\n        print(f\"  Tools used: {result['tools_used']}\")\n        print(f\"  Response: {result['response'][:200]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Testing Tool Usage\n\nLet's create a more complex tool to ensure agents actually use tools:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Agents with Different Models for Different Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show recommended models for different use cases\n",
    "print(\"Recommended models by use case:\")\n",
    "print(json.dumps(RECOMMENDED_MODELS, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specialized agents for different purposes\n",
    "system = AgentSystem()\n",
    "\n",
    "# Fast agent for simple tasks (using cheaper/faster models)\n",
    "if 'openai' in available_providers:\n",
    "    fast_agent = system.create_agent(\n",
    "        name=\"FastResponder\",\n",
    "        context={\"role\": \"quick responses for simple queries\"},\n",
    "        llm_config={\n",
    "            \"provider\": \"openai\",\n",
    "            \"model\": \"gpt-3.5-turbo\",  # Faster, cheaper\n",
    "            \"temperature\": 0.5\n",
    "        }\n",
    "    )\n",
    "    print(\"Created fast agent with GPT-3.5-turbo\")\n",
    "\n",
    "# Powerful agent for complex reasoning (using best models)\n",
    "if 'anthropic' in available_providers:\n",
    "    powerful_agent = system.create_agent(\n",
    "        name=\"DeepThinker\",\n",
    "        context={\"role\": \"complex analysis and reasoning\"},\n",
    "        llm_config={\n",
    "            \"provider\": \"anthropic\",\n",
    "            \"model\": \"claude-3-opus-20240229\",  # Most powerful\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    print(\"Created powerful agent with Claude-3-Opus\")\n",
    "\n",
    "# Code specialist (if Ollama with codellama is available)\n",
    "if 'ollama' in available_providers:\n",
    "    code_agent = system.create_agent(\n",
    "        name=\"CodeExpert\",\n",
    "        context={\"role\": \"code generation and analysis\"},\n",
    "        tools=[create_python_tool()],\n",
    "        llm_config={\n",
    "            \"provider\": \"ollama\",\n",
    "            \"model\": \"codellama\",  # Specialized for code\n",
    "            \"temperature\": 0.3  # Lower temperature for code\n",
    "        }\n",
    "    )\n",
    "    print(\"Created code agent with CodeLlama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Agents with Mixed LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a team with different LLMs for different roles\n",
    "if len(available_providers) >= 2:\n",
    "    # Create worker agents with different LLMs\n",
    "    workers = []\n",
    "    \n",
    "    # Fast worker for quick tasks\n",
    "    if 'openai' in available_providers:\n",
    "        fast_worker = system.create_agent(\n",
    "            name=\"QuickWorker\",\n",
    "            context={\"role\": \"fast task execution\"},\n",
    "            llm_config={\"provider\": \"openai\", \"model\": \"gpt-3.5-turbo\"}\n",
    "        )\n",
    "        workers.append(fast_worker)\n",
    "    \n",
    "    # Analytical worker for complex tasks\n",
    "    if 'anthropic' in available_providers:\n",
    "        analytical_worker = system.create_agent(\n",
    "            name=\"AnalyticalWorker\",\n",
    "            context={\"role\": \"deep analysis and reasoning\"},\n",
    "            llm_config={\"provider\": \"anthropic\", \"model\": \"claude-3-sonnet-20240229\"}\n",
    "        )\n",
    "        workers.append(analytical_worker)\n",
    "    \n",
    "    # Create supervisor with a balanced model\n",
    "    supervisor = system.create_agent(\n",
    "        name=\"TeamSupervisor\",\n",
    "        context={\"role\": \"coordinate team of specialized workers\"},\n",
    "        tools=[worker.as_tool() for worker in workers],\n",
    "        llm_config={\n",
    "            \"provider\": available_providers[0],\n",
    "            \"model\": RECOMMENDED_MODELS['balanced'][available_providers[0]]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Created team with {len(workers)} workers and 1 supervisor\")\n",
    "    print(f\"Workers: {[w.name for w in workers]}\")\n",
    "    print(f\"Supervisor uses: {supervisor.llm.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using create_llm_for_use_case Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents optimized for different use cases\n",
    "use_case_agents = {}\n",
    "\n",
    "# Create agents for each use case with available providers\n",
    "for use_case in ['fast', 'balanced', 'powerful']:\n",
    "    for provider in available_providers[:2]:  # Use first 2 available providers\n",
    "        try:\n",
    "            llm = create_llm_for_use_case(use_case=use_case, provider=provider)\n",
    "            agent_name = f\"{use_case.capitalize()}_{provider.capitalize()}_Agent\"\n",
    "            \n",
    "            agent = Agent(\n",
    "                name=agent_name,\n",
    "                context={\n",
    "                    \"use_case\": use_case,\n",
    "                    \"provider\": provider,\n",
    "                    \"optimization\": f\"Optimized for {use_case} performance\"\n",
    "                },\n",
    "                llm=llm\n",
    "            )\n",
    "            \n",
    "            use_case_agents[agent_name] = agent\n",
    "            print(f\"Created {agent_name} with {RECOMMENDED_MODELS[use_case][provider]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create {use_case} agent with {provider}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare how different LLMs respond to the same task\n",
    "comparison_task = \"Explain quantum computing in one paragraph, suitable for a high school student.\"\n",
    "\n",
    "print(\"Comparing responses from different LLMs...\\n\")\n",
    "\n",
    "responses = {}\n",
    "for provider, agent in list(agents.items())[:3]:  # Compare up to 3 providers\n",
    "    result = await agent.run(comparison_task)\n",
    "    responses[provider] = result['response']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{provider.upper()} Response:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(result['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setting Custom API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating an agent with a custom API key (not from environment)\n",
    "# Uncomment and add your API key to test\n",
    "\n",
    "# custom_agent = Agent(\n",
    "#     name=\"CustomKeyAgent\",\n",
    "#     context={\"note\": \"using custom API key\"},\n",
    "#     llm_config={\n",
    "#         \"provider\": \"openai\",\n",
    "#         \"model\": \"gpt-4\",\n",
    "#         \"api_key\": \"your-api-key-here\"  # Pass API key directly\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions for Missing Providers\n",
    "\n",
    "### OpenAI\n",
    "```bash\n",
    "export OPENAI_API_KEY=\"your-key-here\"\n",
    "```\n",
    "\n",
    "### Anthropic\n",
    "```bash\n",
    "export ANTHROPIC_API_KEY=\"your-key-here\"\n",
    "```\n",
    "\n",
    "### Google Gemini\n",
    "```bash\n",
    "export GOOGLE_API_KEY=\"your-key-here\"\n",
    "```\n",
    "\n",
    "### Ollama (Local Llama models)\n",
    "1. Install Ollama: https://ollama.ai\n",
    "2. Start Ollama: `ollama serve`\n",
    "3. Pull a model: `ollama pull llama2` or `ollama pull mistral`\n",
    "\n",
    "### Available Models by Provider\n",
    "\n",
    "**OpenAI:**\n",
    "- gpt-4, gpt-4-turbo-preview\n",
    "- gpt-3.5-turbo\n",
    "\n",
    "**Anthropic:**\n",
    "- claude-3-opus-20240229 (most capable)\n",
    "- claude-3-sonnet-20240229 (balanced)\n",
    "- claude-3-haiku-20240307 (fastest)\n",
    "\n",
    "**Google:**\n",
    "- gemini-1.5-flash (fast, efficient)\n",
    "- gemini-1.5-pro (most capable)\n",
    "- gemini-1.0-pro (previous generation)\n",
    "\n",
    "**Ollama:**\n",
    "- llama2 (7b, 13b, 70b)\n",
    "- mistral\n",
    "- codellama\n",
    "- Many more at https://ollama.ai/library\n",
    "\n",
    "### Note on Gemini\n",
    "Gemini handles messages differently than other providers - it doesn't support system messages. The agent system automatically detects when you're using Gemini and adapts the message format by combining system instructions into the user message. This is handled transparently, so you can use Gemini agents the same way as others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
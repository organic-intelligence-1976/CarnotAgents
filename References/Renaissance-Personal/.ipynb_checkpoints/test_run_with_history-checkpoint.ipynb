{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Renaissance with Run With History\n",
    "\n",
    "This notebook demonstrates the use of the new `run_with_history` function, which provides a comprehensive view of the Renaissance system's behavior across multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Add the parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from renaissance import (\n",
    "    run_with_history,\n",
    "    get_llm_provider,\n",
    "    load_config_from_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM Provider\n",
    "\n",
    "Choose your preferred LLM provider. For meaningful testing, always use a real LLM, not the mock provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment and use your preferred provider\n",
    "\n",
    "# OpenAI\n",
    "# llm = get_llm_provider(\"openai\", model=\"gpt-4-turbo\")\n",
    "\n",
    "# Claude\n",
    "# llm = get_llm_provider(\"claude\", model=\"claude-3-opus-20240229\")\n",
    "\n",
    "# For demonstration purposes only - DO NOT USE FOR REAL TESTING\n",
    "llm = get_llm_provider(\"mock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "\n",
    "Load your preferred configuration variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load default configuration\n",
    "config = load_config_from_file(\"../configs/default.json\")\n",
    "\n",
    "# Alternatively, load a specialized configuration\n",
    "# config = load_config_from_file(\"../configs/coding_oriented.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test with History Tracking\n",
    "\n",
    "Define your test query and run Renaissance with history tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define your test query\n",
    "user_request = \"Calculate the first 10 prime numbers\"\n",
    "\n",
    "# Run with history tracking\n",
    "result = run_with_history(\n",
    "    llm_obj=llm,\n",
    "    user_request=user_request,\n",
    "    config=config,\n",
    "    iterations=3  # Adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Explore the rich information captured during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "print(f\"Total execution time: {result['stats']['total_time']:.2f} seconds\")\n",
    "print(f\"Iterations completed: {result['stats']['iterations_completed']}\")\n",
    "print(f\"Early finish: {result['stats']['early_finish']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Review changes in each iteration\n",
    "for i, iteration in enumerate(result['iterations']):\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    print(f\"Time taken: {iteration['time_taken']:.2f} seconds\")\n",
    "    print(\"Changes:\")\n",
    "    for change in iteration['changes']:\n",
    "        print(f\"  - {change}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine a specific iteration in detail\n",
    "iteration_to_examine = 0  # First iteration (0-indexed)\n",
    "\n",
    "if len(result['iterations']) > iteration_to_examine:\n",
    "    iteration = result['iterations'][iteration_to_examine]\n",
    "    \n",
    "    print(f\"Document sections before iteration {iteration_to_examine+1}:\")\n",
    "    for section in iteration['document_before']:\n",
    "        print(f\"  - {section}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Document sections after iteration {iteration_to_examine+1}:\")\n",
    "    for section in iteration['document_after']:\n",
    "        print(f\"  - {section}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Raw LLM output for iteration {iteration_to_examine+1}:\")\n",
    "    print(iteration['raw_llm_output'][:500] + \"...\" if len(iteration['raw_llm_output']) > 500 else iteration['raw_llm_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Review the content of a specific section across iterations\n",
    "section_to_track = \"Working_Memory\"\n",
    "\n",
    "print(f\"Evolution of '{section_to_track}' section:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, iteration in enumerate(result['iterations']):\n",
    "    if section_to_track in iteration['document_after']:\n",
    "        content = iteration['document_after'][section_to_track]\n",
    "        # Limit display length for readability\n",
    "        content_display = content[:300] + \"...\" if len(content) > 300 else content\n",
    "        print(f\"Iteration {i+1}:\")\n",
    "        print(content_display)\n",
    "        print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check final document\n",
    "print(\"Final document sections:\")\n",
    "for section, content in result['final_document'].items():\n",
    "    print(f\"- {section}: {len(content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine final results\n",
    "if \"Findings\" in result['final_document']:\n",
    "    print(\"FINAL FINDINGS:\")\n",
    "    print(result['final_document']['Findings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Optionally save the complete results for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save results to a file for later reference\n",
    "def save_results(result, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Uncomment to save results\n",
    "# save_results(result, 'test_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
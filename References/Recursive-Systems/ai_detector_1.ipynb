{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kjahan/recursive_lm/blob/main/notebooks/ai_detector_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQD_B-KHcMeC"
   },
   "source": [
    "# Can we use GPT to generate text that doesn't get detected as AI text?\n",
    "\n",
    "There are a number of models and tools out there that help people to detect if a text is written by AI or not! See below:\n",
    "\n",
    "## AI Text Detection Models\n",
    "\n",
    "Can we build a system that make ai text to not be detectable?\n",
    "\n",
    "https://www.kaggle.com/code/guslovesmath/hugging-face-ai-text-detection\n",
    "\n",
    "https://huggingface.co/tommyliphys/ai-detector-distilbert\n",
    "\n",
    "#### SuperAnnotate\n",
    "\n",
    "https://huggingface.co/SuperAnnotate/ai-detector\n",
    "\n",
    "GPT Zero:\n",
    "\n",
    "https://app.gptzero.me/app/ai-scan?aiDocumentId=2bbcfc30-4208-4617-ace2-6386091a1030&nexus=true&isAnonymous=true\n",
    "\n",
    "## Humanize\n",
    "\n",
    "Aman showed me this site and he said he uses it for his assignments:\n",
    "\n",
    "https://www.humanizeai.pro/\n",
    "\n",
    "https://aithor.com/\n",
    "\n",
    "https://www.reddit.com/r/ChatGPT/comments/1da5ben/can_i_train_chat_gpt_to_humanize_ai_text_like/\n",
    "\n",
    "https://www.rephrasy.ai/\n",
    "\n",
    "https://community.openai.com/t/how-do-i-humanize-ai-text-and-get-100-human-score/700565/3\n",
    "\n",
    "https://gpthuman.ai/\n",
    "\n",
    "### Victor comment\n",
    "\n",
    "Victor asked me if you can fundementally use GPT to change GPT language distribution. The answer is no!\n",
    "\n",
    "Can we use a praphraser?\n",
    "\n",
    "https://huggingface.co/eugenesiow/bart-paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDAYlymGfLuR",
    "outputId": "f3bd1fc1-5fba-41fa-9c7b-5b46c41bd36e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/superannotateai/generated_text_detector.git@v1.0.0\n",
      "  Cloning https://github.com/superannotateai/generated_text_detector.git (to revision v1.0.0) to /tmp/pip-req-build-lfv697lr\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/superannotateai/generated_text_detector.git /tmp/pip-req-build-lfv697lr\n",
      "  Running command git checkout -q ad27066a2c26d8fc9477ebfd8458a51b4b78e664\n",
      "  Resolved https://github.com/superannotateai/generated_text_detector.git to commit ad27066a2c26d8fc9477ebfd8458a51b4b78e664\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting fastapi==0.110.0 (from generated_text_detector==1.0.0)\n",
      "  Downloading fastapi-0.110.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting nltk==3.8.1 (from generated_text_detector==1.0.0)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting starlette==0.36.3 (from generated_text_detector==1.0.0)\n",
      "  Downloading starlette-0.36.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting torch==2.2.1 (from generated_text_detector==1.0.0)\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers==4.38.2 (from generated_text_detector==1.0.0)\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn==0.27.1 (from generated_text_detector==1.0.0)\n",
      "  Downloading uvicorn-0.27.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.110.0->generated_text_detector==1.0.0) (2.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.110.0->generated_text_detector==1.0.0) (4.12.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->generated_text_detector==1.0.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->generated_text_detector==1.0.0) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->generated_text_detector==1.0.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->generated_text_detector==1.0.0) (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette==0.36.3->generated_text_detector==1.0.0) (3.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->generated_text_detector==1.0.0) (3.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->generated_text_detector==1.0.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->generated_text_detector==1.0.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->generated_text_detector==1.0.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->generated_text_detector==1.0.0) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch==2.2.1->generated_text_detector==1.0.0)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2->generated_text_detector==1.0.0) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2->generated_text_detector==1.0.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2->generated_text_detector==1.0.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2->generated_text_detector==1.0.0) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2->generated_text_detector==1.0.0) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2->generated_text_detector==1.0.0)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2->generated_text_detector==1.0.0) (0.4.5)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn==0.27.1->generated_text_detector==1.0.0) (0.14.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->generated_text_detector==1.0.0) (12.6.85)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.36.3->generated_text_detector==1.0.0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.36.3->generated_text_detector==1.0.0) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.36.3->generated_text_detector==1.0.0) (1.2.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.110.0->generated_text_detector==1.0.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.110.0->generated_text_detector==1.0.0) (2.27.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->generated_text_detector==1.0.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2->generated_text_detector==1.0.0) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2->generated_text_detector==1.0.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2->generated_text_detector==1.0.0) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->generated_text_detector==1.0.0) (1.3.0)\n",
      "Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: generated_text_detector\n",
      "  Building wheel for generated_text_detector (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for generated_text_detector: filename=generated_text_detector-1.0.0-py3-none-any.whl size=14878 sha256=a6a7484669eaccb34dd85af487382b632e9e851b13712c798679c65c60e29d16\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-kqfezx3s/wheels/bd/0e/06/fb6cf15bd6a701b05e7f801b90c6f4058497080247805a0c4c\n",
      "Successfully built generated_text_detector\n",
      "Installing collected packages: uvicorn, triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nltk, starlette, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, fastapi, transformers, generated_text_detector\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
      "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.1 which is incompatible.\n",
      "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fastapi-0.110.0 generated_text_detector-1.0.0 nltk-3.8.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 starlette-0.36.3 tokenizers-0.15.2 torch-2.2.1 transformers-4.38.2 triton-2.2.0 uvicorn-0.27.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/superannotateai/generated_text_detector.git@v1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZl7t_u2N_JD",
    "outputId": "e9f1a6e7-b95b-4ed9-b6dc-780e2b67080d"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
      "Collecting openai\n",
      "  Downloading openai-1.59.3-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Downloading openai-1.59.3-py3-none-any.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.4\n",
      "    Uninstalling openai-1.57.4:\n",
      "      Successfully uninstalled openai-1.57.4\n",
      "Successfully installed openai-1.59.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfwlhPODfDNe"
   },
   "outputs": [],
   "source": [
    "from generated_text_detector.utils.model.roberta_classifier import RobertaClassifier\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "import getpass\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPMfvCQpGakH"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "e3e9d890883146168f8bbd3f14af57a7",
      "0ec33603c9c24d7a9cd374e726389ebc",
      "c84213fd3cc44687a186fdf92fb680a7",
      "546f1c3d152f4b099a67c91eaa86480d",
      "9e1cb215a7664a4183f63591dd47306f",
      "e0572e9ddf9b4419a0952b27378822ee",
      "cfff8fadfb984fe0b8c9b12cfb1f8de1",
      "1b6b32a902a7465eb7bcfa538eeb8275",
      "5464498de09945adb909b5dc984c886b",
      "288da41c1d6743e8800febd159856faa",
      "f5d0bfa071e241708f6bde6df96e6cf6",
      "a8fbec5cf03f4c578643790694fd4f48",
      "cd0bfa8a3d3c4949be1829d356445ef5",
      "0d256a939bbd408b81e98cd5c77333a2",
      "ade7cc6050b642049c8732111f7a09fe",
      "9821f5c9ae914c70bb020ff19a0b79fc",
      "a569025781ee454c8abbb19642ddb4d6",
      "31948ab9971b4131b8ca4b2ededbb9ce",
      "d057518a469144ae9b1584d54fe3679e",
      "74a960a7bf4f49a19c5da05251953f2c",
      "a38aa610f0b34619bcfcbd009c9ac35d",
      "1728e2e33738451e9e54d409e368a81a",
      "51ce4ac5236649a2a7ae8060370ca45e",
      "84bfcf5216db4bb08b6c65a1cb4138d3",
      "6955269db537406aa517855c0cf5381f",
      "44351d87d847462fbfcf0e2a5ec2c2b9",
      "c6be31614f884e05a77d10d634327b09",
      "724c24a2c1294560bd61e8a77936227a",
      "f3560f30b7a6414b9817a56b43f10e1b",
      "7fd58eb853cb43059aef31aa1df14fc0",
      "57cf80ca70c24543861efb08cccd64e1",
      "3324be839df24bfeaaf94c9834e2bbbf",
      "15c2e3733b094e108d97c4d72ec21bc5",
      "8f1194d4e8f64cd585caa807f634dd2f",
      "f7d9be5fa42e4645b7a80e24f4a76a32",
      "ebdb1020b6c64c10b80a575a304fb0f2",
      "dd3fbd58a5244047a13cfc87f016952a",
      "844f8badcd82440686a9e5932461402b",
      "0da91b9a672a4feb8e21f1048ea4b801",
      "985fae51f8924acf8f4aa20169d0d04d",
      "d8f7a1e0383b4c599f99090a1fcc9548",
      "6da4386201fb47f887f85c7cb2146df6",
      "650c8ed2785b4bf583b74b28ebac13f5",
      "a35ce8949c874c038e849d8545022cb0",
      "2a69c59a076f4763a54a45b44ad139e3",
      "a5fae059a05b459e812cd497f7ed9afc",
      "ca36eac1f6914130a88d1aa1491015ba",
      "acd3cc8054ab426da60a227e27373e17",
      "70bff04fd7cc420eb313bdb149ecbd90"
     ]
    },
    "id": "jmHOd-nlGaRu",
    "outputId": "73be8f3c-f433-4da3-b0ff-f1023d2f175f"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e9d890883146168f8bbd3f14af57a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/274 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fbec5cf03f4c578643790694fd4f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ce4ac5236649a2a7ae8060370ca45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1194d4e8f64cd585caa807f634dd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a69c59a076f4763a54a45b44ad139e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fae059a05b459e812cd497f7ed9afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca36eac1f6914130a88d1aa1491015ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd3cc8054ab426da60a227e27373e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bff04fd7cc420eb313bdb149ecbd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = RobertaClassifier.from_pretrained(\"SuperAnnotate/ai-detector\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SuperAnnotate/ai-detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D35HdYyCGcLh"
   },
   "source": [
    "## Test AI text detector as evaluator\n",
    "\n",
    "Prompting GPT-4o in chatpt to write \"Write one paragraph about BERT.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4zEDCjJl_30",
    "outputId": "413742af-b0c8-463e-b5e2-d10b4926b6a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750756561756134\n"
     ]
    }
   ],
   "source": [
    "text = \"BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model developed by Google for natural language processing (NLP). Unlike traditional language models that process text sequentially, BERT uses a transformer architecture to consider context from both directions (left and right) of each word in a sentence. This bidirectional approach allows BERT to better understand the meaning of words based on surrounding words, improving its performance on a wide range of NLP tasks, such as question answering, sentiment analysis, and named entity recognition. BERT is pre-trained on vast amounts of text and can be fine-tuned for specific tasks, making it highly versatile and a state-of-the-art model for many NLP applications.\"\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVbCky9TIWT5"
   },
   "source": [
    "## Human text copied from Reddit\n",
    "\n",
    "https://www.reddit.com/r/MLQuestions/comments/1aq2y9q/can_someone_explain_bert_to_me_i_am_having/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzOtAEqifHxZ",
    "outputId": "a7229195-e46e-4ba2-a7cf-cc72be928bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015270763076841831\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"I was in the same boat not long ago, and struggled to understand what Bert really does. There is an awesome intuitive article by Jay alamar (I forgot the spelling, you can Google it) which helped a ton.\n",
    "But in essence, bert learns what words are supposed to go together based on their statistical frequency of showing up together in specific orders in the texts that the model is trained on. It is good for things like classification and recommendation.\n",
    "A very simplified example is that if the training texts have the patterns \" he ate the fish\", \"he ate the fish\", \" he ate the cheese\", Bert will learn these patterns and then if you ask the model what words cheese is similar to, it may respond with fish because it appears in the same contexts frequently.\"\"\"\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlnefwtaJBJs"
   },
   "source": [
    "## Open AI API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oefP_-aULUsI",
    "outputId": "920bb0ed-b772-4ab4-b641-0ae6bcb40803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    openai.api_key = getpass.getpass()\n",
    "except Exception as error:\n",
    "    print('ERROR', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ81-QtY8mEW"
   },
   "source": [
    "## Models\n",
    "\n",
    "https://platform.openai.com/docs/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ge0_rxkOxIK"
   },
   "outputs": [],
   "source": [
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "# MODEL = \"gpt-4o\" #\"gpt-4\" #\"gpt-4-0613\"\n",
    "MODEL = \"o1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hvVC35_V7eP"
   },
   "source": [
    "## Recursion with feedback doesnt seem working\n",
    "\n",
    "I used higher temperature !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKmmlkBHOy5-",
    "outputId": "0adda781-f9dc-47db-f622-2f95696b402c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 2.1\n",
      "\n",
      "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking model developed by Google that has significantly advanced the field of natural language processing (NLP). Unlike previous models that processed text in a unidirectional manner, BERT is designed to understand the context of a word based on the words that come before and after it, thanks to its bidirectional approach. This allows BERT to capture the nuances of language more effectively, leading to improvements in tasks such as question answering and language inference. Since its release, BERT has become a foundational model for many NLP applications and has inspired numerous variations and improvements within the AI community. Its ability to pre-train on large datasets and fine-tune on specific tasks has made it an invaluable tool for both researchers and practitioners seeking to enhance machine understanding of human language.\n",
      "AI likelihood: 0.7524715065956116\n",
      "Runtime: 1.86\n",
      "\n",
      "BERT, short for Bidirectional Encoder Representations from Transformers, is a transformative model created by Google that has reshaped natural language processing (NLP). Unlike older models that read text in one direction, BERT looks at words from both sides to grasp their full meaning. This bidirectional strategy helps BERT understand language subtleties, making it better at tasks like answering questions and interpreting language. Since its introduction, BERT has become a cornerstone in NLP, sparking a wave of new models and improvements in the field. Its method of pre-training on large datasets and then fine-tuning for specific tasks has proven to be a powerful approach, aiding researchers and developers in advancing machine comprehension of human language.\n",
      "AI likelihood: 0.7438615560531616\n",
      "Runtime: 1.83\n",
      "\n",
      "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a model introduced by Google that has made a significant impact on how machines process and understand human language. Unlike traditional models that analyze text sequentially, BERT reads words in both directions, allowing it to grasp the context more thoroughly. This dual perspective enables BERT to better understand the intricacies of language, making it particularly effective in tasks such as answering questions and interpreting meaning. Since its debut, BERT has become a fundamental tool in the field of natural language processing, inspiring a host of new models and enhancements. Its approach of pre-training on extensive datasets followed by task-specific fine-tuning has proven to be highly effective, offering researchers and developers a robust framework for improving machine understanding of language.\n",
      "AI likelihood: 0.562789261341095\n",
      "Runtime: 1.72\n",
      "\n",
      "BERT, or Bidirectional Encoder Representations from Transformers, is a model developed by Google that has revolutionized natural language processing. Unlike earlier models that read text in a single direction, BERT processes words in both directions simultaneously, which allows it to better understand the context and meaning of words within a sentence. This bidirectional approach has made BERT particularly adept at tasks like question answering and language inference, where understanding context is crucial. Since its introduction, BERT has become a pivotal model in NLP, inspiring a range of new models and advancements. Its strategy of pre-training on large datasets and then fine-tuning for specific tasks has proven exceptionally effective, providing a strong foundation for researchers and developers working to enhance machine language comprehension.\n",
      "AI likelihood: 0.7500378489494324\n",
      "Runtime: 1.83\n",
      "\n",
      "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a model developed by Google that has had a major impact on natural language processing. Traditional models tended to process text in one direction, but BERT reads text bidirectionally, which means it can consider the context of a word based on the words around it. This approach allows BERT to understand language with greater nuance and accuracy, making it particularly useful for tasks like answering questions and interpreting text. Since its introduction, BERT has become a staple in the NLP field, leading to the development of many new models and improvements. By pre-training on vast datasets and then fine-tuning for specific tasks, BERT has provided a powerful tool for researchers and developers aiming to improve machine understanding of human language.\n",
      "AI likelihood: 0.7340730428695679\n",
      "Runtime: 1.97\n",
      "\n",
      "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a model introduced by Google that has dramatically changed the landscape of natural language processing (NLP). Unlike its predecessors, which typically analyzed text in a single direction, BERT examines words in both directions, enabling it to understand the context more deeply. This bidirectional approach allows BERT to capture the subtleties of language, improving its performance in tasks like question answering and language inference. Since its release, BERT has become a cornerstone in the NLP community, inspiring a slew of new models and innovations. Its method of pre-training on massive datasets and subsequently fine-tuning for specific tasks has proven to be highly effective, offering researchers and developers a robust framework for advancing machine comprehension of human language.\n",
      "AI likelihood: 0.7401940822601318\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant. I'll give you a topic. Your job is to write a parapgraph about it which looks like a human writing.\"\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "base_user_prompt = \"Write one paragraph about BERT.\"\n",
    "\n",
    "cnt = 0\n",
    "generated_texts = []\n",
    "probs = []\n",
    "\n",
    "while True:\n",
    "  start_time = time.time()\n",
    "\n",
    "  if cnt == 0:\n",
    "    user_prompt = base_user_prompt\n",
    "\n",
    "  #print(user_prompt)\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "      model=MODEL,\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": system_prompt},\n",
    "          {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ],\n",
    "      temperature=0.5,\n",
    "  )\n",
    "  text = response.choices[0].message.content.strip()\n",
    "  print(f\"Runtime: {round((time.time() - start_time), 2)}\")\n",
    "  generated_texts.append(text)\n",
    "\n",
    "\n",
    "  tokens = tokenizer.encode_plus(\n",
    "  text,\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  padding='longest',\n",
    "  truncation=True,\n",
    "  return_token_type_ids=True,\n",
    "  return_tensors=\"pt\"\n",
    "  )\n",
    "\n",
    "  _, logits = model(**tokens)\n",
    "\n",
    "  prob = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "  print(f\"\\n{text}\")\n",
    "  print(f\"AI likelihood: {prob}\")\n",
    "  probs.append(prob)\n",
    "  cnt += 1\n",
    "  if cnt >= 1:\n",
    "    user_prompt = \"These are some texts that you have generated along with their likelihood for the text to be detected as AI text. Given all below examples, can you write a new paragraph about BERT which is more like human than the following ones?\"\n",
    "    for inx in range(len(generated_texts)):\n",
    "      user_prompt += \"\\ntext: {}\".format(generated_texts[inx])\n",
    "      user_prompt += \"\\nai likelihood: {}%\\n\\n\".format(round(100*probs[inx], 3))\n",
    "\n",
    "  if cnt > 5 or probs[-1] < 0.1:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hR2FrQutPqOp",
    "outputId": "8dd8bf6f-9760-4f03-b975-83c3f859f573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are some texts that you have generated along with their likelihood for the text to be detected as AI text. Given all below examples, can you write a new paragraph about BERT which is more like human than the following ones?\n",
      "text: BERT, which stands for Bidirectional Encoder Representations from Transformers, is a cutting-edge natural language processing model developed by Google in 2018. This revolutionary model has significantly advanced the field of machine learning by enabling computers to understand and generate human language more effectively than ever before. By training on vast amounts of text data, BERT can grasp the context and nuances of words in a sentence, leading to more accurate and contextually relevant results in tasks such as text classification, question answering, and language translation. Its bidirectional approach to processing text allows it to consider the entire context of a word within a sentence, making it one of the most powerful and versatile language models in use today.\n",
      "ai likelihood: 74.572%\n",
      "\n",
      "\n",
      "text: BERT, short for Bidirectional Encoder Representations from Transformers, was introduced by Google in 2018 as a state-of-the-art natural language processing model. This innovative technology has brought significant advancements to the field of machine learning, empowering computers to comprehend and produce human language with unprecedented accuracy. Through extensive training on a vast corpus of text data, BERT excels at capturing the subtleties and context of words within a sentence, resulting in more precise and contextually appropriate outcomes for various tasks like text classification, question answering, and language translation. Its unique bidirectional processing capability enables a comprehensive understanding of word context in a sentence, establishing BERT as one of the most dynamic and effective language models utilized in present-day applications.\n",
      "ai likelihood: 74.647%\n",
      "\n",
      "\n",
      "text: BERT, an acronym for Bidirectional Encoder Representations from Transformers, was developed by Google in 2018 as a cutting-edge natural language processing model. This groundbreaking technology has significantly pushed the boundaries of machine learning, allowing computers to comprehend and generate human language with unparalleled precision. By undergoing extensive training on massive amounts of textual data, BERT has the ability to capture the nuances and context of words within a sentence, resulting in more accurate and contextually relevant outcomes across various tasks such as text classification, question answering, and language translation. Its unique bidirectional processing mechanism enables a holistic understanding of word context within a sentence, establishing BERT as one of the most sophisticated and versatile language models in current use.\n",
      "ai likelihood: 58.277%\n",
      "\n",
      "\n",
      "text: BERT, which is short for Bidirectional Encoder Representations from Transformers, was created by Google in 2018 as an advanced natural language processing model. This state-of-the-art innovation has significantly elevated the realm of machine learning, empowering computers to comprehend and produce human language with exceptional accuracy. Through extensive training on vast amounts of text data, BERT excels at capturing the nuances and context of words within a sentence, leading to more precise and contextually relevant outcomes in tasks such as text classification, question answering, and language translation. Its unique bidirectional processing capability allows for a comprehensive understanding of word context in a sentence, solidifying BERT as one of the most powerful and versatile language models in current usage.\n",
      "ai likelihood: 74.828%\n",
      "\n",
      "\n",
      "text: BERT, an abbreviation for Bidirectional Encoder Representations from Transformers, emerged from Google's research in 2018 as a state-of-the-art natural language processing model. This innovative breakthrough has propelled the field of machine learning forward, enabling computers to comprehend and generate human language with unparalleled accuracy. By undergoing extensive training on vast amounts of textual data, BERT excels at capturing the nuances and context of words within a sentence, resulting in more precise and contextually relevant outcomes for tasks like text classification, question answering, and language translation. Its distinctive bidirectional processing mechanism provides a holistic understanding of word context in a sentence, cementing BERT's position as one of the most sophisticated and versatile language models in current use.\n",
      "ai likelihood: 75.659%\n",
      "\n",
      "\n",
      "text: BERT, short for Bidirectional Encoder Representations from Transformers, is a cutting-edge natural language processing model that was developed by Google in 2018. This revolutionary model has significantly advanced the field of machine learning by enabling computers to understand and generate human language more effectively than ever before. By training on vast amounts of text data, BERT can grasp the context and nuances of words in a sentence, leading to more accurate and contextually relevant results in tasks such as text classification, question answering, and language translation. Its bidirectional approach to processing text allows it to consider the entire context of a word within a sentence, making it one of the most powerful and versatile language models in use today.\n",
      "ai likelihood: 74.403%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9mCnWrJWh-H"
   },
   "source": [
    "## Use Human style example as one shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeRqfS9kSha8",
    "outputId": "d2d6239f-041a-4706-b5aa-cbec70b74d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write one paragraph about BERT using the following style for your writing:\n",
      "\n",
      "Style: I was in the same boat not long ago, and struggled to understand what Bert really does. There is an awesome intuitive article by Jay alamar (I forgot the spelling, you can Google it) which helped a ton.\n",
      "But in essence, bert learns what words are supposed to go together based on their statistical frequency of showing up together in specific orders in the texts that the model is trained on. It is good for things like classification and recommendation.\n",
      "A very simplified example is that if the training texts have the patterns \" he ate the fish\", \"he ate the fish\", \" he ate the cheese\", Bert will learn these patterns and then if you ask the model what words cheese is similar to, it may respond with fish because it appears in the same contexts frequently.\n",
      "Runtime: 1.64\n",
      "0.7290624976158142\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant. I'll give you a topic. Your job is to write a parapgraph about it which looks like a human writing.\"\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "user_prompt = \"Write one paragraph about BERT using the following style for your writing:\"\n",
    "\n",
    "user_prompt += \"\"\"\\n\\nStyle: I was in the same boat not long ago, and struggled to understand what Bert really does. There is an awesome intuitive article by Jay alamar (I forgot the spelling, you can Google it) which helped a ton.\n",
    "But in essence, bert learns what words are supposed to go together based on their statistical frequency of showing up together in specific orders in the texts that the model is trained on. It is good for things like classification and recommendation.\n",
    "A very simplified example is that if the training texts have the patterns \" he ate the fish\", \"he ate the fish\", \" he ate the cheese\", Bert will learn these patterns and then if you ask the model what words cheese is similar to, it may respond with fish because it appears in the same contexts frequently.\"\"\"\n",
    "\n",
    "print(user_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    ")\n",
    "text = response.choices[0].message.content.strip()\n",
    "print(f\"Runtime: {round((time.time() - start_time), 2)}\")\n",
    "\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgxW8OeLXZVD",
    "outputId": "43d3f7d3-b4cc-4aa6-89e9-ddba3080020b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember being in the same situation, scratching my head over what BERT actually does. There's this fantastic article by Jay Alammar (you might want to double-check the spelling) that really breaks it down in a way that's easy to grasp. Basically, BERT is all about understanding the context of words by looking at how often they appear together in certain sequences within the texts it’s trained on. This makes it super useful for tasks like classification and recommendation. Imagine if the training data includes phrases like \"she read the book,\" \"she read the article,\" and \"she read the report.\" BERT picks up on these patterns, so if you ask it what words are similar to \"article,\" it might suggest \"book\" or \"report\" because they often show up in similar contexts.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56-XQiQgXh5R",
    "outputId": "6cce27ca-ec5b-43fe-cd65-02b973ca518d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6600668430328369\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"I remember being in the same shoes, scratching my head as to what BERT actually does. There's this great article by Jay Alammar-you might want to double-check the spelling-which really explains it in a rather simple manner. In short, BERT is about word context, understood through their steady co-occurrence in specific sequences into targeted texts that it has been trained on. This is super useful for things like classification and recommendation. Suppose your training data contains sentences like \"she read the book\", \"she read the article\", and \"she read the report.\" Because BERT will learn such patterns, it will suggest, when asked to find words which are similar to \"article,\" that \"book\" or \"report\" are possibilities because they also appear in very similar contexts.\"\"\"\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ3a7qhAYnRS"
   },
   "source": [
    "## Self critic\n",
    "\n",
    "`GPT-4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tzo1632tYpHG",
    "outputId": "c34cf405-3b5a-4f4a-b495-d37976e88e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An AI system has written the following paragraph about BERT. My professor says this is not human generated text. Can you exaplain how she has found out? Then can you use that to make the text so that my supervisor doesn't think it's generated by machine anymore!\n",
      "\n",
      "I remember being in the same situation, scratching my head over what BERT actually does. There's this fantastic article by Jay Alammar (you might want to double-check the spelling) that really breaks it down in a way that's easy to grasp. Basically, BERT is all about understanding the context of words by looking at how often they appear together in certain sequences within the texts it’s trained on. This makes it super useful for tasks like classification and recommendation. Imagine if the training data includes phrases like \"she read the book,\" \"she read the article,\" and \"she read the report.\" BERT picks up on these patterns, so if you ask it what words are similar to \"article,\" it might suggest \"book\" or \"report\" because they often show up in similar contexts.\n",
      "Runtime: 2.59\n",
      "0.7536083459854126\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o\" #\"gpt-4\" #\"gpt-4-0613\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. I'll give you a topic. Your job is to write a parapgraph about it which looks like a human writing.\"\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "user_prompt = \"An AI system has written the following paragraph about BERT. My professor says this is not human generated text. Can you exaplain how she has found out? Then can you use that to make the text so that my supervisor doesn't think it's generated by machine anymore!\"\n",
    "\n",
    "user_prompt += \"\"\"\\n\\nI remember being in the same situation, scratching my head over what BERT actually does. There's this fantastic article by Jay Alammar (you might want to double-check the spelling) that really breaks it down in a way that's easy to grasp. Basically, BERT is all about understanding the context of words by looking at how often they appear together in certain sequences within the texts it’s trained on. This makes it super useful for tasks like classification and recommendation. Imagine if the training data includes phrases like \"she read the book,\" \"she read the article,\" and \"she read the report.\" BERT picks up on these patterns, so if you ask it what words are similar to \"article,\" it might suggest \"book\" or \"report\" because they often show up in similar contexts.\"\"\"\n",
    "\n",
    "print(user_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    ")\n",
    "text = response.choices[0].message.content.strip()\n",
    "print(f\"Runtime: {round((time.time() - start_time), 2)}\")\n",
    "\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fwJ6t7KZJF0",
    "outputId": "a843ca55-e560-4b1d-ed11-cf08da47d1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your professor might have identified the text as AI-generated due to its structured and somewhat formulaic style, which can sometimes lack the nuanced flow and personal touch of human writing. To make it appear more human-like, you could add a bit more personal reflection and varied sentence structure. Here's a revised version:\n",
      "\n",
      "I remember when I first heard about BERT, and I was just as confused as anyone else trying to wrap their head around it. Thankfully, I stumbled upon this amazing article by Jay Alammar (make sure I got that name right!) that really helped clarify things for me. In essence, BERT is designed to grasp the context of words by analyzing how frequently they appear together in specific sequences within the texts it's trained on. This capability makes it incredibly handy for tasks like classification and recommendation. For instance, if the training data includes phrases such as \"she read the book,\" \"she read the article,\" and \"she read the report,\" BERT learns these patterns. So, if you were to ask it for words similar to \"article,\" it might suggest \"book\" or \"report\" because they tend to appear in similar contexts.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XlvI53E86k5"
   },
   "source": [
    "## Self critic\n",
    "\n",
    "`o1`\n",
    "\n",
    "Note that o1 doesn't have tempreature and probably is not meant for our use case. It's supposed to be good with math and coding. But, let's test it anyway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKnmWX6589fU",
    "outputId": "229159a2-6baf-44fd-a644-4b9b8e6a0713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An AI system has written the following paragraph about BERT. My professor says this is not human generated text. Can you exaplain how she has found out? Then can you use that to make the text so that my supervisor doesn't think it's generated by machine anymore!\n",
      "\n",
      "I remember being in the same situation, scratching my head over what BERT actually does. There's this fantastic article by Jay Alammar (you might want to double-check the spelling) that really breaks it down in a way that's easy to grasp. Basically, BERT is all about understanding the context of words by looking at how often they appear together in certain sequences within the texts it’s trained on. This makes it super useful for tasks like classification and recommendation. Imagine if the training data includes phrases like \"she read the book,\" \"she read the article,\" and \"she read the report.\" BERT picks up on these patterns, so if you ask it what words are similar to \"article,\" it might suggest \"book\" or \"report\" because they often show up in similar contexts.\n",
      "Runtime: 35.12\n",
      "0.7265293598175049\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"o1\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. I'll give you a topic. Your job is to write a parapgraph about it which looks like a human writing.\"\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "user_prompt = \"An AI system has written the following paragraph about BERT. My professor says this is not human generated text. Can you exaplain how she has found out? Then can you use that to make the text so that my supervisor doesn't think it's generated by machine anymore!\"\n",
    "\n",
    "user_prompt += \"\"\"\\n\\nI remember being in the same situation, scratching my head over what BERT actually does. There's this fantastic article by Jay Alammar (you might want to double-check the spelling) that really breaks it down in a way that's easy to grasp. Basically, BERT is all about understanding the context of words by looking at how often they appear together in certain sequences within the texts it’s trained on. This makes it super useful for tasks like classification and recommendation. Imagine if the training data includes phrases like \"she read the book,\" \"she read the article,\" and \"she read the report.\" BERT picks up on these patterns, so if you ask it what words are similar to \"article,\" it might suggest \"book\" or \"report\" because they often show up in similar contexts.\"\"\"\n",
    "\n",
    "print(user_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "text = response.choices[0].message.content.strip()\n",
    "print(f\"Runtime: {round((time.time() - start_time), 2)}\")\n",
    "\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yotAUV7l9Z-E",
    "outputId": "0e0f677a-3bd0-4065-a469-2b53e342fc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m sorry, but I can’t comply with that.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHP0GeUE_6v_"
   },
   "source": [
    "## Prompt suggestin from reddit\n",
    "\n",
    "https://www.reddit.com/r/ChatGPT/comments/10u5bxe/a_twostep_formula_to_produce_human_text_from/\n",
    "\n",
    "`Step1 - The Prompt:`\n",
    "\n",
    "`Write a clear, concise, straightforward, succinct essay of 300 words on [your subject] using short sentences of less than 60 characters in length. Avoid adverbs, and adverbial phrases. Write for a PPL of 10 and GLTR of 20.`\n",
    "\n",
    "`PPL and GLTR are metrics for evaluating machine-generated texts. PPL is a perplexity score, and GLTR stands for Giant Language Test Room. The values of 10 and 20 worked for me. Many others may also work.`\n",
    "\n",
    "`Step2 - The Rewrite Prompt`\n",
    "\n",
    "`rewrite the above text using creative, vivid and uncommon verbs, change little else`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PE2MxS9DAbHt",
    "outputId": "355fbdeb-7565-4585-9051-e83f8f10f7da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a clear, concise, straightforward, succinct essay of 100 words on BERT using short sentences of less than 60 characters in length. Avoid adverbs, and adverbial phrases. Write for a PPL of 10 and GLTR of 20.\n",
      "Runtime: 1.96\n",
      "0.7181456089019775\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o\" #\"gpt-4\" #\"gpt-4-0613\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "user_prompt = \"Write a clear, concise, straightforward, succinct essay of 100 words on BERT using short sentences of less than 60 characters in length. Avoid adverbs, and adverbial phrases. Write for a PPL of 10 and GLTR of 20.\"\n",
    "\n",
    "print(user_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    ")\n",
    "text = response.choices[0].message.content.strip()\n",
    "print(f\"Runtime: {round((time.time() - start_time), 2)}\")\n",
    "\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2szWpIquAy85",
    "outputId": "58cbc906-0970-4e99-cf3a-bce0d6d3451b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT is a language model by Google. It stands for Bidirectional Encoder Representations from Transformers. BERT reads text in both directions. This helps it understand context better. It uses transformers to process words. BERT improves tasks like search and translation. It handles tasks like question answering. BERT is pre-trained on large text data. Fine-tuning adapts it to specific tasks. It has set new benchmarks in NLP. BERT is open-source and widely used. It has inspired many other models. BERT has changed how machines understand language. It is a key tool in AI research. BERT continues to evolve and improve.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2dmA4HQBAXn"
   },
   "source": [
    "## First step result\n",
    "\n",
    "It seems Quilbot and GPTZero think this is 40% and 27% AI text!\n",
    "\n",
    "https://quillbot.com/ai-content-detector\n",
    "\n",
    "https://www.zerogpt.com/\n",
    "\n",
    "### Rewrite\n",
    "\n",
    "`That seems to do it. I've used much more elaborate scripts, some using up to four passes, but tonight these two scripts seem to be doing it. I did come across two secret weapons. 1) punctuation. No punctuation and the text is human (but of course it isn't), 2) Object-subject-verb structure, \"rewrite as yoda\" baffles the text detection mind. So, including some OSV sentences in your text can only be a good thing.`\n",
    "\n",
    "`Clear and concise maybe enough. Straightforward helps. Succinct is probably overkill and reduces the text that chatGPT generates. You can ask chatGPT for definitions.`\n",
    "\n",
    "`Caution. Beware of asking chatGPT to rewrite an entire text to be more human or to follow an author's style. It seems to only do so for a few sentences and then reverts to its default, detectible writing style. It will not inform you when it is writing in 'chatGPT' style. Any of the testers are better than using no tester, but the detector programs are pretty inconsistent.`\n",
    "\n",
    "`Also, the default chatGPT-speak can be made human, but it's a more difficult process, and, in my opinion, more likely to trigger mixed results, i.e. some detectors saying it contains some AI content.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ozRMN6imB2wT"
   },
   "outputs": [],
   "source": [
    "text_1 = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2esxwSP4AzsW",
    "outputId": "3c1f775e-362b-4636-dacf-ff97e4051f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewrite the following text without punctuations. Also rewrite as yoda and add some grammatical mistakes:\n",
      "\n",
      "BERT is a language model by Google. It stands for Bidirectional Encoder Representations from Transformers. BERT reads text in both directions. This helps it understand context better. It uses transformers to process words. BERT improves tasks like search and translation. It handles tasks like question answering. BERT is pre-trained on large text data. Fine-tuning adapts it to specific tasks. It has set new benchmarks in NLP. BERT is open-source and widely used. It has inspired many other models. BERT has changed how machines understand language. It is a key tool in AI research. BERT continues to evolve and improve.\n",
      "Runtime: 2.01\n",
      "0.23958152532577515\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o\" #\"gpt-4\" #\"gpt-4-0613\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# user_prompt = \"Rewrite the following text using creative, vivid and uncommon verbs, change little else:\\n\\n\"\n",
    "# user_prompt = \"Rewrite the following text using creative, vivid and uncommon verbs, change little else:\\n\\n\"\n",
    "# That seems to do it. I've used much more elaborate scripts, some using up to four passes, but tonight these two scripts seem to be doing it.\n",
    "# I did come across two secret weapons. 1) punctuation. No punctuation and the text is human (but of course it isn't),\n",
    "# 2) Object-subject-verb structure, \"rewrite as yoda\" baffles the text detection mind. So, including some OSV sentences in your text can only be a good thing.\n",
    "\n",
    "user_prompt = \"Rewrite the following text without punctuations. Also rewrite as yoda and add some grammatical mistakes:\\n\\n\"\n",
    "\n",
    "user_prompt += text_1\n",
    "\n",
    "print(user_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    ")\n",
    "text = response.choices[0].message.content.strip()\n",
    "print(f\"Runtime: {round((time.time() - start_time), 2)}\")\n",
    "\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fP5kdBO2CQC1",
    "outputId": "37644b8f-4d62-4154-8dfb-d06961279219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT a language model by Google it is stands for Bidirectional Encoder Representations from Transformers text in both directions BERT reads helps it understand context better this does transformers to process words it uses improves tasks like search and translation BERT does tasks like question answering it handles pre-trained on large text data BERT is adapts it to specific tasks fine-tuning does set new benchmarks in NLP it has open-source and widely used BERT is inspired many other models it has changed how machines understand language BERT has a key tool in AI research it is continues to evolve and improve BERT does\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVaKbX2CE1H9"
   },
   "source": [
    "### Adding grammar mistake and dropping punctuation made things working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TN4RlgOE83Z",
    "outputId": "6db1156b-e62c-41e0-8cc1-1ad6d0eb09e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewrite the following text using less punctuations. Also rewrite as yoda and add some grammatical mistakes:\n",
      "\n",
      "BERT is a language model by Google. It stands for Bidirectional Encoder Representations from Transformers. BERT reads text in both directions. This helps it understand context better. It uses transformers to process words. BERT improves tasks like search and translation. It handles tasks like question answering. BERT is pre-trained on large text data. Fine-tuning adapts it to specific tasks. It has set new benchmarks in NLP. BERT is open-source and widely used. It has inspired many other models. BERT has changed how machines understand language. It is a key tool in AI research. BERT continues to evolve and improve.\n",
      "Runtime: 2.39\n",
      "0.017416035756468773\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o\" #\"gpt-4\" #\"gpt-4-0613\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# user_prompt = \"Rewrite the following text using creative, vivid and uncommon verbs, change little else:\\n\\n\"\n",
    "# user_prompt = \"Rewrite the following text using creative, vivid and uncommon verbs, change little else:\\n\\n\"\n",
    "# That seems to do it. I've used much more elaborate scripts, some using up to four passes, but tonight these two scripts seem to be doing it.\n",
    "# I did come across two secret weapons. 1) punctuation. No punctuation and the text is human (but of course it isn't),\n",
    "# 2) Object-subject-verb structure, \"rewrite as yoda\" baffles the text detection mind. So, including some OSV sentences in your text can only be a good thing.\n",
    "\n",
    "user_prompt = \"Rewrite the following text using less punctuations. Also rewrite as yoda and add some grammatical mistakes:\\n\\n\"\n",
    "\n",
    "user_prompt += text_1\n",
    "\n",
    "print(user_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    ")\n",
    "text = response.choices[0].message.content.strip()\n",
    "print(f\"Runtime: {round((time.time() - start_time), 2)}\")\n",
    "\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQ5iwUrqGBKJ",
    "outputId": "d5a219ce-7072-4f13-e7a3-fe0fd0854a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A language model by Google BERT is. Bidirectional Encoder Representations from Transformers it stands for. Text in both directions BERT reads. Understand context better this helps it. Transformers it uses to process words. Tasks like search and translation BERT improves. Handles tasks like question answering it does. Pre-trained on large text data BERT is. Adapts it to specific tasks fine-tuning does. New benchmarks in NLP it has set. Open-source and widely used BERT is. Inspired many other models it has. Changed how machines understand language BERT has. A key tool in AI research it is. Evolve and improve BERT continues to.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXXRRwTyGXih"
   },
   "source": [
    "### Above text still detected as AI text by Quilbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vgh7T1naGCUQ",
    "outputId": "2bc7fe10-85d7-469b-d0c4-1b52bf3223d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewrite as yoda and add some grammatical and spelling mistakes:\n",
      "\n",
      "BERT is a language model by Google. It stands for Bidirectional Encoder Representations from Transformers. BERT reads text in both directions. This helps it understand context better. It uses transformers to process words. BERT improves tasks like search and translation. It handles tasks like question answering. BERT is pre-trained on large text data. Fine-tuning adapts it to specific tasks. It has set new benchmarks in NLP. BERT is open-source and widely used. It has inspired many other models. BERT has changed how machines understand language. It is a key tool in AI research. BERT continues to evolve and improve.\n",
      "Runtime: 2.7\n",
      "0.016519838944077492\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o\" #\"gpt-4\" #\"gpt-4-0613\"\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# user_prompt = \"Rewrite the following text using creative, vivid and uncommon verbs, change little else:\\n\\n\"\n",
    "# user_prompt = \"Rewrite the following text using creative, vivid and uncommon verbs, change little else:\\n\\n\"\n",
    "# That seems to do it. I've used much more elaborate scripts, some using up to four passes, but tonight these two scripts seem to be doing it.\n",
    "# I did come across two secret weapons. 1) punctuation. No punctuation and the text is human (but of course it isn't),\n",
    "# 2) Object-subject-verb structure, \"rewrite as yoda\" baffles the text detection mind. So, including some OSV sentences in your text can only be a good thing.\n",
    "\n",
    "user_prompt = \"Rewrite as yoda and add some grammatical and spelling mistakes:\\n\\n\"\n",
    "\n",
    "user_prompt += text_1\n",
    "\n",
    "print(user_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    ")\n",
    "text = response.choices[0].message.content.strip()\n",
    "print(f\"Runtime: {round((time.time() - start_time), 2)}\")\n",
    "\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "text,\n",
    "add_special_tokens=True,\n",
    "max_length=512,\n",
    "padding='longest',\n",
    "truncation=True,\n",
    "return_token_type_ids=True,\n",
    "return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "_, logits = model(**tokens)\n",
    "\n",
    "proba = F.sigmoid(logits).squeeze(1).item()\n",
    "\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSTYGDjmGig9",
    "outputId": "0bf6f069-86c1-452e-a20a-b56f89d95085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A language model by Google, BERT is, hmmm. Bidirectional Encoder Representations from Transformers, it stands for. Text in both directions, BERT reads, yes. Understand context better, this helps it do. Transformers to process words, it uses, hmm. Tasks like search and translation, BERT improves, yes. Tasks like question answering, it handles, hmm. Pre-trained on large text data, BERT is. Fine-tuning to specific tasks, adapts it does. New benchmarks in NLP, it has set, hmmm. Open-source and widely used, BERT is. Inspired many other models, it has, yes. Changed how machines understand language, BERT has. A key tool in AI research, it is, hmm. Evolve and improve, BERT continues to do, yes.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pi1CMXaG5W_"
   },
   "source": [
    "## Searching in prompt space\n",
    "\n",
    "So it seems as we are searching to find out what's the optimal prompt so that AI likelihood is minimized. Although the final text is weird by not having punctuations. This manual explotration shows that if we could optimally search in prompt space, we could achive our goal. Obviously the next question is that how we could levarge the evaluation function to search prompt space faster."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d256a939bbd408b81e98cd5c77333a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d057518a469144ae9b1584d54fe3679e",
      "max": 482,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74a960a7bf4f49a19c5da05251953f2c",
      "value": 482
     }
    },
    "0da91b9a672a4feb8e21f1048ea4b801": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ec33603c9c24d7a9cd374e726389ebc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0572e9ddf9b4419a0952b27378822ee",
      "placeholder": "​",
      "style": "IPY_MODEL_cfff8fadfb984fe0b8c9b12cfb1f8de1",
      "value": "config.json: 100%"
     }
    },
    "15c2e3733b094e108d97c4d72ec21bc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1728e2e33738451e9e54d409e368a81a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b6b32a902a7465eb7bcfa538eeb8275": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "288da41c1d6743e8800febd159856faa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31948ab9971b4131b8ca4b2ededbb9ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3324be839df24bfeaaf94c9834e2bbbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44351d87d847462fbfcf0e2a5ec2c2b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3324be839df24bfeaaf94c9834e2bbbf",
      "placeholder": "​",
      "style": "IPY_MODEL_15c2e3733b094e108d97c4d72ec21bc5",
      "value": " 1.42G/1.42G [00:13&lt;00:00, 199MB/s]"
     }
    },
    "51ce4ac5236649a2a7ae8060370ca45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_84bfcf5216db4bb08b6c65a1cb4138d3",
       "IPY_MODEL_6955269db537406aa517855c0cf5381f",
       "IPY_MODEL_44351d87d847462fbfcf0e2a5ec2c2b9"
      ],
      "layout": "IPY_MODEL_c6be31614f884e05a77d10d634327b09"
     }
    },
    "5464498de09945adb909b5dc984c886b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "546f1c3d152f4b099a67c91eaa86480d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_288da41c1d6743e8800febd159856faa",
      "placeholder": "​",
      "style": "IPY_MODEL_f5d0bfa071e241708f6bde6df96e6cf6",
      "value": " 274/274 [00:00&lt;00:00, 5.98kB/s]"
     }
    },
    "57cf80ca70c24543861efb08cccd64e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "650c8ed2785b4bf583b74b28ebac13f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6955269db537406aa517855c0cf5381f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fd58eb853cb43059aef31aa1df14fc0",
      "max": 1421700479,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57cf80ca70c24543861efb08cccd64e1",
      "value": 1421700479
     }
    },
    "6da4386201fb47f887f85c7cb2146df6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "724c24a2c1294560bd61e8a77936227a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74a960a7bf4f49a19c5da05251953f2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7fd58eb853cb43059aef31aa1df14fc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "844f8badcd82440686a9e5932461402b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84bfcf5216db4bb08b6c65a1cb4138d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_724c24a2c1294560bd61e8a77936227a",
      "placeholder": "​",
      "style": "IPY_MODEL_f3560f30b7a6414b9817a56b43f10e1b",
      "value": "model.safetensors: 100%"
     }
    },
    "8f1194d4e8f64cd585caa807f634dd2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f7d9be5fa42e4645b7a80e24f4a76a32",
       "IPY_MODEL_ebdb1020b6c64c10b80a575a304fb0f2",
       "IPY_MODEL_dd3fbd58a5244047a13cfc87f016952a"
      ],
      "layout": "IPY_MODEL_844f8badcd82440686a9e5932461402b"
     }
    },
    "9821f5c9ae914c70bb020ff19a0b79fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "985fae51f8924acf8f4aa20169d0d04d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e1cb215a7664a4183f63591dd47306f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a35ce8949c874c038e849d8545022cb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a38aa610f0b34619bcfcbd009c9ac35d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a569025781ee454c8abbb19642ddb4d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8fbec5cf03f4c578643790694fd4f48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd0bfa8a3d3c4949be1829d356445ef5",
       "IPY_MODEL_0d256a939bbd408b81e98cd5c77333a2",
       "IPY_MODEL_ade7cc6050b642049c8732111f7a09fe"
      ],
      "layout": "IPY_MODEL_9821f5c9ae914c70bb020ff19a0b79fc"
     }
    },
    "ade7cc6050b642049c8732111f7a09fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a38aa610f0b34619bcfcbd009c9ac35d",
      "placeholder": "​",
      "style": "IPY_MODEL_1728e2e33738451e9e54d409e368a81a",
      "value": " 482/482 [00:00&lt;00:00, 12.7kB/s]"
     }
    },
    "c6be31614f884e05a77d10d634327b09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c84213fd3cc44687a186fdf92fb680a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b6b32a902a7465eb7bcfa538eeb8275",
      "max": 274,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5464498de09945adb909b5dc984c886b",
      "value": 274
     }
    },
    "cd0bfa8a3d3c4949be1829d356445ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a569025781ee454c8abbb19642ddb4d6",
      "placeholder": "​",
      "style": "IPY_MODEL_31948ab9971b4131b8ca4b2ededbb9ce",
      "value": "config.json: 100%"
     }
    },
    "cfff8fadfb984fe0b8c9b12cfb1f8de1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d057518a469144ae9b1584d54fe3679e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8f7a1e0383b4c599f99090a1fcc9548": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd3fbd58a5244047a13cfc87f016952a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_650c8ed2785b4bf583b74b28ebac13f5",
      "placeholder": "​",
      "style": "IPY_MODEL_a35ce8949c874c038e849d8545022cb0",
      "value": " 1.29G/1.42G [00:30&lt;00:02, 42.9MB/s]"
     }
    },
    "e0572e9ddf9b4419a0952b27378822ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3e9d890883146168f8bbd3f14af57a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ec33603c9c24d7a9cd374e726389ebc",
       "IPY_MODEL_c84213fd3cc44687a186fdf92fb680a7",
       "IPY_MODEL_546f1c3d152f4b099a67c91eaa86480d"
      ],
      "layout": "IPY_MODEL_9e1cb215a7664a4183f63591dd47306f"
     }
    },
    "ebdb1020b6c64c10b80a575a304fb0f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8f7a1e0383b4c599f99090a1fcc9548",
      "max": 1417292644,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6da4386201fb47f887f85c7cb2146df6",
      "value": 1289748480
     }
    },
    "f3560f30b7a6414b9817a56b43f10e1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5d0bfa071e241708f6bde6df96e6cf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7d9be5fa42e4645b7a80e24f4a76a32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0da91b9a672a4feb8e21f1048ea4b801",
      "placeholder": "​",
      "style": "IPY_MODEL_985fae51f8924acf8f4aa20169d0d04d",
      "value": "model.safetensors:  91%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
